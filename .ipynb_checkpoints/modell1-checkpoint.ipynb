{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e5aef0-c3c1-4e98-aff5-4aefe61337c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/torch/lib/python3.7/site-packages (1.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/torch/lib/python3.7/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/torch/lib/python3.7/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/envs/torch/lib/python3.7/site-packages (from pandas) (1.17.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/torch/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.13.0)\n",
      "\u001b[33mDEPRECATION: tensorflow-serving-api 2.2.0 has a non-standard dependency specifier grpcio>=1.0<2. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of tensorflow-serving-api or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/torch/lib/python3.7/site-packages (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /opt/conda/envs/torch/lib/python3.7/site-packages (from scikit-learn) (1.17.5)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /opt/conda/envs/torch/lib/python3.7/site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/torch/lib/python3.7/site-packages (from scikit-learn) (0.13.2)\n",
      "\u001b[33mDEPRECATION: tensorflow-serving-api 2.2.0 has a non-standard dependency specifier grpcio>=1.0<2. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of tensorflow-serving-api or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Installation\n",
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e466879d-2c13-4f6e-9284-19ab29da7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate requirements.txt\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95dabfb1-c469-4a5e-9e1c-c279f3999122",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "event indicator must be binary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m efs_event \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mefs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCensoring\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m})\n\u001b[1;32m     17\u001b[0m efs_time \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mefs_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mSurv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mefs_event\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mefs_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Entferne Zielvariablen aus den Features\u001b[39;00m\n\u001b[1;32m     21\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mefs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mefs_time\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/envs/survival-env/lib/python3.9/site-packages/sksurv/util.py:64\u001b[0m, in \u001b[0;36mSurv.from_arrays\u001b[0;34m(event, time, name_event, name_time)\u001b[0m\n\u001b[1;32m     62\u001b[0m events\u001b[38;5;241m.\u001b[39msort()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(events) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent indicator must be binary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(events \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mevents\u001b[38;5;241m.\u001b[39mdtype)):\n\u001b[1;32m     67\u001b[0m     y[name_event] \u001b[38;5;241m=\u001b[39m event\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: event indicator must be binary"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sksurv.column import encode_categorical\n",
    "from sksurv.preprocessing import OneHotEncoder as SurvivalOneHotEncoder\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.util import Surv\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "\n",
    "# Laden des Datensatzes\n",
    "current_dir = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "df = pd.read_csv(current_dir + \"/data/train.csv\")\n",
    "\n",
    "# Zielvariable definieren (Ereignis und Zeit)\n",
    "df['efs'] = df['efs'].map({'Event': 1, 'Censoring': 0})  # Sicherstellen, dass die Werte binär sind\n",
    "efs_event = df['efs'].astype(bool)\n",
    "efs_time = df['efs_time']\n",
    "y = Surv.from_arrays(event=efs_event, time=efs_time)\n",
    "\n",
    "# Entferne Zielvariablen aus den Features\n",
    "X = df.drop(columns=['efs', 'efs_time'])\n",
    "\n",
    "# Numerische und kategoriale Spalten identifizieren\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Fehlende Werte ersetzen\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
    "X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
    "\n",
    "# One-Hot-Encoding für kategoriale Variablen\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_cat = encoder.fit_transform(X[cat_cols])\n",
    "X_cat = pd.DataFrame(X_cat, columns=encoder.get_feature_names_out(cat_cols))\n",
    "\n",
    "# Standardisierung der numerischen Variablen\n",
    "scaler = StandardScaler()\n",
    "X_num = pd.DataFrame(scaler.fit_transform(X[num_cols]), columns=num_cols)\n",
    "\n",
    "# Kombinieren von numerischen und kodierten kategorialen Features\n",
    "X_processed = pd.concat([X_num, X_cat], axis=1)\n",
    "\n",
    "# Survival-Modell erstellen\n",
    "rsf = RandomSurvivalForest(n_estimators=100, min_samples_split=10, min_samples_leaf=10, random_state=42)\n",
    "rsf.fit(X_processed, y)\n",
    "\n",
    "# Vorhersage des \"Risk Scores\"\n",
    "predictions = rsf.predict(X_processed)\n",
    "\n",
    "# Speichern der Ergebnisse\n",
    "results = pd.DataFrame({'ID': df.index, 'prediction': predictions})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15893b46-bfdd-4206-98ec-dbcc6c7fa36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (survival-env)",
   "language": "python",
   "name": "survival-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
